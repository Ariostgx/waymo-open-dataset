{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2Tgtmr1rhZS"
      },
      "source": [
        "# Overview: Waymo Open Dataset -- Perception Object Assets\n",
        "\n",
        "Modeling the 3D world from sensor data for simulation is a scalable way of developing testing and validation environments for robotic learning problems such as autonomous driving. We provide a large-scale object-centric asset dataset containing over 520K images and lidar observations of two major categories (vehicles and pedestrians) from the released Perception data (v2.0.0). We hope this data will enable and advance research on 3D point cloud reconstruction and completion, object NeRF reconstruction, and generative object assets to address the real-world driving challenges with occlusions, lighting-variations, and long-tail distributions.\n",
        "\n",
        "Please familiarize yourself with the [Perception data v2 format and tutorial](https://github.com/waymo-research/waymo-open-dataset) when proceed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVGBfyvyraAg"
      },
      "outputs": [],
      "source": [
        "#@title Initial setup\n",
        "from typing import Optional, Any, Mapping, Tuple\n",
        "import warnings\n",
        "# Disable annoying warnings from PyArrow using under the hood.\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import dask.dataframe as dd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import plotly  # Used by visu3d\n",
        "import tensorflow as tf\n",
        "import visu3d\n",
        "\n",
        "from waymo_open_dataset import v2\n",
        "from waymo_open_dataset.utils import camera_segmentation_utils\n",
        "\n",
        "# Path to the directory with all components\n",
        "dataset_dir = '\u003cspecify actual path\u003e'\n",

        "\n",
        "context_name = '2736377008667623133_2676_410_2696_410'\n",
        "\n",
        "def read(tag: str) -\u003e dd.DataFrame:\n",
        "  \"\"\"Creates a Dask DataFrame for the component specified by its tag.\"\"\"\n",
        "  paths = tf.io.gfile.glob(f'{dataset_dir}/{tag}/{context_name}.parquet')\n",
        "  return dd.read_parquet(paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALoVdVGWLCQ1"
      },
      "source": [
        "## Load and visualize camera fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGjN4T1yLGpT"
      },
      "outputs": [],
      "source": [
        "#@title Visualization utitlity\n",
        "def apply_color_mask(image: np.ndarray,\n",
        "                     mask: np.ndarray,\n",
        "                     color: Tuple[int, int, int],\n",
        "                     alpha: float = 0.5) -\u003e np.ndarray:\n",
        "  \"\"\"Applies the given mask to the image.\"\"\"\n",
        "  color = np.array(color)[np.newaxis, :]\n",
        "  bg = image * (1 - alpha) + alpha * color\n",
        "  output = np.where(mask, bg, image).astype(np.uint8)\n",
        "  return output\n",
        "\n",
        "\n",
        "def grid_imshow(h: int, w: int, images: Any) -\u003e None:\n",
        "  \"\"\"Displays images in a grid.\"\"\"\n",
        "  fig, axes = plt.subplots(h, w)\n",
        "  fig.set_size_inches(20, 10)\n",
        "  fig.tight_layout()\n",
        "  for i, image in enumerate(images):\n",
        "    ax = axes[i] if len(images) \u003e 0 else axes\n",
        "    ax.imshow(image)\n",
        "    ax.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbiIT2nm61g1"
      },
      "outputs": [],
      "source": [
        "#@title Basic Example (Camera images with rays and labels)\n",
        "\n",
        "asset_camera_sensor_df = read('object_asset_camera_sensor')\n",
        "asset_ray_df = read('object_asset_ray')\n",
        "asset_auto_label_df = read('object_asset_auto_label')\n",
        "# Load additional LiDAR box dimensions to obtain the ray-box intersection\n",
        "laser_box_df = read('lidar_box')\n",
        "\n",
        "asset_df = v2.merge(asset_camera_sensor_df, asset_ray_df)\n",
        "asset_df = v2.merge(asset_df, asset_auto_label_df)\n",
        "asset_df = v2.merge(asset_df, laser_box_df)\n",
        "\n",
        "# Show raw data\n",
        "asset_df.head()\n",
        "\n",
        "def parse_key(r: Any) -\u003e Mapping[str, Any]:\n",
        "  \"\"\"Parses key from each row.\"\"\"\n",
        "  lidar_box_component = v2.LiDARBoxComponent.from_dict(r)\n",
        "  return {\n",
        "      'segment_context_name': lidar_box_component.key.segment_context_name,\n",
        "      'laser_object_id': lidar_box_component.key.laser_object_id,\n",
        "      'frame_timestamp_micros': lidar_box_component.key.frame_timestamp_micros,\n",
        "  }\n",
        "\n",
        "def parse_lidar_box(r: Any) -\u003e Mapping[str, Any]:\n",
        "  \"\"\"Parses lidar box from each row.\"\"\"\n",
        "  lidar_box_component = v2.LiDARBoxComponent.from_dict(r)\n",
        "  lidar_box = lidar_box_component.box\n",
        "  lidar_box_3d = np.asarray(\n",
        "      [lidar_box.center.x,\n",
        "       lidar_box.center.y,\n",
        "       lidar_box.center.z,\n",
        "       lidar_box.size.x,\n",
        "       lidar_box.size.y,\n",
        "       lidar_box.size.z,\n",
        "       lidar_box.heading], dtype=np.float64)\n",
        "  return {'box_3d': lidar_box_3d}\n",
        "\n",
        "\n",
        "def parse_camera_sensor(r: Any) -\u003e Mapping[str, Any]:\n",
        "  \"\"\"Parses camera sensor data from each row.\"\"\"\n",
        "  camera_sensor_component = v2.ObjectAssetCameraSensorComponent.from_dict(r)\n",
        "  rgb_image = tf.image.decode_png(camera_sensor_component.rgb_image).numpy()\n",
        "  proj_points_mask = tf.image.decode_png(\n",
        "      camera_sensor_component.proj_points_mask).numpy()\n",
        "  return {\n",
        "      'rgb_image': rgb_image,\n",
        "      'proj_points_mask': proj_points_mask}\n",
        "\n",
        "\n",
        "def parse_camera_ray(r: Any) -\u003e Mapping[str, Any]:\n",
        "  \"\"\"Parses camera ray data from each row.\"\"\"\n",
        "  box_fields = parse_lidar_box(r)\n",
        "\n",
        "  ray_component = v2.ObjectAssetRayComponent.from_dict(r)\n",
        "  ray_origin = ray_component.ray_origin.tensor.numpy()\n",
        "  ray_direction = ray_component.ray_direction.tensor.numpy()\n",
        "\n",
        "  im_height, im_width = ray_origin.shape[:2]\n",
        "  ray_mask, _, _ = v2._object_asset_utils.get_ray_box_intersects(\n",
        "      ray_origin.reshape(-1, 3),\n",
        "      ray_direction.reshape(-1, 3),\n",
        "      box_fields['box_3d'][3:6],\n",
        "  )\n",
        "\n",
        "  ray_mask = ray_mask.reshape(im_height, im_width, -1)\n",
        "\n",
        "  return {'ray_origin': ray_origin, 'ray_direction': ray_direction, 'ray_mask': ray_mask}\n",
        "\n",
        "\n",
        "def parse_camera_label(r: Any) -\u003e Mapping[str, Any]:\n",
        "  \"\"\"Parses camera auto label from each row.\"\"\"\n",
        "  auto_label_component = v2.ObjectAssetAutoLabelComponent.from_dict(r)\n",
        "  obj_mask = tf.image.decode_png(\n",
        "      auto_label_component.object_mask).numpy()\n",
        "  semantic_mask = tf.image.decode_png(\n",
        "      auto_label_component.semantic_mask).numpy()\n",
        "  instance_mask = tf.image.decode_png(\n",
        "      auto_label_component.instance_mask, dtype=tf.uint16).numpy()\n",
        "  return {\n",
        "      'obj_mask': obj_mask,\n",
        "      'semantic_mask':semantic_mask,\n",
        "      'instance_mask': instance_mask}\n",
        "\n",
        "\n",
        "# Example how to access data fields.\n",
        "print(f'Available {asset_df.shape[0].compute()} rows:')\n",
        "for i, (_, r) in enumerate(asset_df.iterrows()):\n",
        "  # Create component dataclasses for the raw data\n",
        "  data_fields = {}\n",
        "  data_fields.update(parse_key(r))\n",
        "  data_fields.update(parse_lidar_box(r))\n",
        "  data_fields.update(parse_camera_sensor(r))\n",
        "  data_fields.update(parse_camera_ray(r))\n",
        "  data_fields.update(parse_camera_label(r))\n",
        "\n",
        "  panoptic_image = camera_segmentation_utils.panoptic_label_to_rgb(\n",
        "      semantic_label=data_fields['semantic_mask'],\n",
        "      instance_label=data_fields['instance_mask'])\n",
        "\n",
        "  print(\n",
        "      'context_name: ', data_fields['segment_context_name'],\n",
        "      ' ts: ', data_fields['frame_timestamp_micros'],\n",
        "      ' laser_object_id: ', data_fields['laser_object_id'])\n",
        "  grid_imshow(1, 5, [\n",
        "      data_fields['rgb_image'],\n",
        "      apply_color_mask(\n",
        "          data_fields['rgb_image'], \n",
        "          data_fields['proj_points_mask'],\n",
        "          color=(0, 255, 0),\n",
        "          alpha=0.7),\n",
        "      apply_color_mask(\n",
        "          data_fields['rgb_image'],\n",
        "          data_fields['ray_mask'],\n",
        "          color=(255, 255, 0),\n",
        "          alpha=0.3),\n",
        "      apply_color_mask(\n",
        "          data_fields['rgb_image'],\n",
        "          data_fields['obj_mask'],\n",
        "          color=(0, 255, 255),\n",
        "          alpha=0.3),\n",
        "          panoptic_image])\n",
        "  plt.show()\n",
        "  if i \u003e 2:\n",
        "    print('...')\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5J1J8rZSCFF"
      },
      "source": [
        "## Load and visualize point clouds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXDDu7uAaM8g"
      },
      "outputs": [],
      "source": [
        "#@title Example to visualize lidar points.\n",
        "asset_df = read('object_asset_lidar_sensor')\n",
        "\n",
        "all_points_xyz = []\n",
        "for i, (_, r) in enumerate(asset_df.iterrows()):\n",
        "  # Create component dataclasses for the raw data\n",
        "  lidar_sensor_component = v2.ObjectAssetLiDARSensorComponent.from_dict(r)\n",
        "  print(\n",
        "      f'context_name: {lidar_sensor_component.key.segment_context_name}',\n",
        "      f' ts: {lidar_sensor_component.key.frame_timestamp_micros}',\n",
        "      f' laser_object_id: {lidar_sensor_component.key.laser_object_id}')\n",
        "  \n",
        "  points_xyz = lidar_sensor_component.points_xyz.tensor.numpy()\n",
        "  all_points_xyz.append(points_xyz)\n",
        "  if i \u003e 2:\n",
        "    break\n",
        "\n",
        "v3d_point_cloud = visu3d.Point3d(\n",
        "    p=np.concatenate(all_points_xyz, axis=0),\n",
        ")\n",
        "v3d_point_cloud.fig"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "tutorial_object_asset.ipynb?workspaceId=xcyan:fig-export-waymo_moving_camera-3354-change-2::citc",
          "timestamp": 1685118968296
        },
        {
          "file_id": "tutorial_object_asset.ipynb?workspaceId=xcyan:fig-export-waymo_moving_camera-3354-change-2::citc",
          "timestamp": 1685118149325
        },
        {
          "file_id": "tutorial_object_asset.ipynb?workspaceId=xcyan:fig-export-waymo_moving_camera-3354-change-2::citc",
          "timestamp": 1685116517636
        },
        {
          "file_id": "1ecyFgu4RDkJSOCZsD6zKTkClGHeA50u3",
          "timestamp": 1685076916736
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
